version: '3.7'

x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflowdb:5432/airflowdb
  - AIRFLOW__CORE__FERNET_KEY=YlCImzjge_TeZc7jPJ7Jz2pgOtb4yTssA1pVyqIADWg=
  - AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
  - AIRFLOW__CORE__DEFAULT_TIMEZONE=America/Bogota
  - AIRFLOW_CONN_SPARK_CONN_ID=spark://spark-master:7077/?queue=root.default&deploy_mode=cluster&spark_home=%2Fopt%2Fbitnami%2Fspark%2F&spark_binary=spark-submit

x-airflow-image: &airflow_image apache/airflow:2.5.0

networks:
  airflow-pyspark:
    driver: bridge
    name: airflow-pyspark
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/24
          gateway: 172.28.0.1

services:

  fetch-data-nasa:
    build:
      context: services/fetch-data
      dockerfile: Dockerfile.nasadata
      args:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    image: fetch-data-nasa
    container_name: fetchdatanasa
    restart: "no"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.3
    volumes:
      - /tmp/dockerdata/:/tmp/
  
  spark-master:
    build:
      context: services/pyspark
      dockerfile: Dockerfile.pyspark
      args:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no 
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.4
    volumes:
      - ./src/:/opt/bitnami/spark/src/
      - ./tests/:/opt/bitnami/spark/tests/
    links:
      - airflow_scheduler
  
  spark-worker-1:
    build:
      context: services/pyspark
      dockerfile: Dockerfile.pyspark
      args:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker 
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no 
      - SPARK_RPC_ENCRYPTION_ENABLED=no 
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8081"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.5
    volumes:
      - ./src/:/opt/bitnami/spark/src/
      - ./tests/:/opt/bitnami/spark/tests/
    depends_on:
      - spark-master
    links:
      - airflow_scheduler
  
  spark-worker-2:
    build:
      context: services/pyspark
      dockerfile: Dockerfile.pyspark
      args:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker 
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no 
      - SPARK_RPC_ENCRYPTION_ENABLED=no 
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8082:8081"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.6
    volumes:
      - ./src/:/opt/bitnami/spark/src/
      - ./tests/:/opt/bitnami/spark/tests/
    depends_on:
      - spark-master
    links:
      - airflow_scheduler
  
  airflowdb:
    image: postgres
    container_name: airflowdb
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
    ports:
      - "5432:5432"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.7
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  airflow_init:
    build:
      context: services/airflow
      dockerfile: Dockerfile.airflow
      args: 
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_init
    depends_on:
      - airflowdb
    environment: *airflow_environment
    restart: unless-stopped
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.8
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname user --lastname test --role Admin --email admin@example.com && airflow db upgrade'

  airflow_webserver:
    build:
      context: services/airflow
      dockerfile: Dockerfile.airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_webserver
    depends_on:
      - airflow_init
    restart: always
    ports:
      - "8083:8080"
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.9
    volumes:
      - logs:/opt/airflow/logs
    environment: *airflow_environment
    command: webserver
  
  airflow_scheduler:
    build:
      context: services/airflow
      dockerfile: Dockerfile.airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_scheduler
    depends_on:
      - airflow_init
    restart: always
    volumes:
      - logs:/opt/airflow/logs
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/spark-apps/
      - //var/run/docker.sock:/var/run/docker.sock
    environment: *airflow_environment
    command: scheduler
    networks:
      airflow-pyspark:
        ipv4_address: 172.28.0.10

volumes:
  postgres-db-volume:
  logs: